{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EEG Motor Imagery Model Comparison\n",
        "\n",
        "This notebook compares different deep learning models (EEGNet, ATCNet, etc.) for EEG motor imagery classification using Leave-One-Subject-Out (LOSO) cross-validation.\n",
        "\n",
        "**Sections:**\n",
        "1. Setup & Configuration\n",
        "2. Preprocessed Data Visualization\n",
        "3. Training Curves (Averaged Over Folds)\n",
        "4. Detailed Test Performance\n",
        "5. Model Comparison Dashboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure matplotlib for clean visuals\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 12\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Configuration\n",
        "\n",
        "Define experiment directories for each model. To add a new model, simply add an entry to `EXPERIMENTS`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - Edit these paths to match your experiments\n",
        "# ============================================================\n",
        "\n",
        "EXPERIMENTS: Dict[str, str] = {\n",
        "    \"EEGNet\": \"../reports/loso/eegnet_experiment\",\n",
        "    \"ATCNet\": \"../reports/loso/atcnet_experiment\",\n",
        "    # Add future models here:\n",
        "    # \"ShallowConvNet\": \"../reports/loso/shallowconv_experiment\",\n",
        "    # \"DeepConvNet\": \"../reports/loso/deepconv_experiment\",\n",
        "}\n",
        "\n",
        "DATA_DIR = Path(\"../data/processed\")\n",
        "SUBJECTS = [f\"A0{i}T\" for i in range(1, 10)]  # A01T to A09T\n",
        "CLASS_NAMES = [\"Left Hand\", \"Right Hand\"]\n",
        "\n",
        "# Aesthetic configuration\n",
        "MODEL_COLORS = {\n",
        "    \"EEGNet\": \"#2E86AB\",     # Steel blue\n",
        "    \"ATCNet\": \"#A23B72\",     # Raspberry\n",
        "    \"ShallowConvNet\": \"#F18F01\",  # Orange\n",
        "    \"DeepConvNet\": \"#C73E1D\",     # Red\n",
        "}\n",
        "\n",
        "def get_model_color(model_name: str) -> str:\n",
        "    \"\"\"Get color for model, with fallback for unknown models.\"\"\"\n",
        "    return MODEL_COLORS.get(model_name, \"#666666\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def load_fold_histories(experiment_dir: Path) -> Dict[str, List[Dict]]:\n",
        "    \"\"\"Load training history from all folds in an experiment.\n",
        "    \n",
        "    Returns:\n",
        "        Dict mapping subject ID to list of epoch records.\n",
        "    \"\"\"\n",
        "    histories = {}\n",
        "    for subject in SUBJECTS:\n",
        "        history_path = experiment_dir / f\"fold_{subject}\" / \"history.json\"\n",
        "        if history_path.exists():\n",
        "            with open(history_path) as f:\n",
        "                histories[subject] = json.load(f)\n",
        "    return histories\n",
        "\n",
        "\n",
        "def load_fold_predictions(experiment_dir: Path) -> Dict[str, Dict[str, np.ndarray]]:\n",
        "    \"\"\"Load test predictions from all folds in an experiment.\n",
        "    \n",
        "    Returns:\n",
        "        Dict mapping subject ID to dict with 'y_pred' and 'y_true' arrays.\n",
        "    \"\"\"\n",
        "    predictions = {}\n",
        "    for subject in SUBJECTS:\n",
        "        pred_path = experiment_dir / f\"fold_{subject}\" / \"test_predictions.npz\"\n",
        "        if pred_path.exists():\n",
        "            data = np.load(pred_path, allow_pickle=True)\n",
        "            predictions[subject] = {\n",
        "                \"y_pred\": data[\"y_pred\"],\n",
        "                \"y_true\": data[\"y_true\"],\n",
        "            }\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def load_summary(experiment_dir: Path) -> Optional[Dict]:\n",
        "    \"\"\"Load LOSO summary JSON.\"\"\"\n",
        "    summary_path = experiment_dir / \"loso_summary.json\"\n",
        "    if summary_path.exists():\n",
        "        with open(summary_path) as f:\n",
        "            return json.load(f)\n",
        "    return None\n",
        "\n",
        "\n",
        "def aggregate_histories(\n",
        "    histories: Dict[str, List[Dict]], \n",
        "    metric: str,\n",
        "    pad_strategy: str = \"final\"\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Aggregate a metric across all folds, handling variable epoch lengths.\n",
        "    \n",
        "    Args:\n",
        "        histories: Dict mapping subject to list of epoch dicts\n",
        "        metric: Key to extract (e.g., 'train_loss', 'val_acc')\n",
        "        pad_strategy: 'final' pads with last value, 'nan' pads with NaN\n",
        "        \n",
        "    Returns:\n",
        "        epochs: Array of epoch numbers [1, 2, ..., max_epochs]\n",
        "        mean: Mean value per epoch\n",
        "        std: Std value per epoch\n",
        "    \"\"\"\n",
        "    if not histories:\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "    \n",
        "    # Extract metric series from each fold\n",
        "    series_list = []\n",
        "    for subject, history in histories.items():\n",
        "        values = [epoch_dict[metric] for epoch_dict in history]\n",
        "        series_list.append(values)\n",
        "    \n",
        "    # Find max length\n",
        "    max_len = max(len(s) for s in series_list)\n",
        "    \n",
        "    # Pad to equal length\n",
        "    padded = []\n",
        "    for series in series_list:\n",
        "        if len(series) < max_len:\n",
        "            if pad_strategy == \"final\":\n",
        "                pad_value = series[-1]\n",
        "            else:\n",
        "                pad_value = np.nan\n",
        "            series = series + [pad_value] * (max_len - len(series))\n",
        "        padded.append(series)\n",
        "    \n",
        "    arr = np.array(padded)  # (n_folds, max_epochs)\n",
        "    epochs = np.arange(1, max_len + 1)\n",
        "    \n",
        "    if pad_strategy == \"final\":\n",
        "        mean = arr.mean(axis=0)\n",
        "        std = arr.std(axis=0)\n",
        "    else:\n",
        "        mean = np.nanmean(arr, axis=0)\n",
        "        std = np.nanstd(arr, axis=0)\n",
        "    \n",
        "    return epochs, mean, std\n",
        "\n",
        "\n",
        "def compute_aggregate_metrics(predictions: Dict[str, Dict]) -> Dict:\n",
        "    \"\"\"Compute aggregate metrics from all fold predictions.\n",
        "    \n",
        "    Returns:\n",
        "        Dict with per-fold accuracies, confusion matrix, and classification report.\n",
        "    \"\"\"\n",
        "    all_y_true = []\n",
        "    all_y_pred = []\n",
        "    fold_accuracies = {}\n",
        "    \n",
        "    for subject, data in predictions.items():\n",
        "        y_true = data[\"y_true\"]\n",
        "        y_pred = data[\"y_pred\"]\n",
        "        all_y_true.extend(y_true)\n",
        "        all_y_pred.extend(y_pred)\n",
        "        fold_accuracies[subject] = (y_true == y_pred).mean()\n",
        "    \n",
        "    all_y_true = np.array(all_y_true)\n",
        "    all_y_pred = np.array(all_y_pred)\n",
        "    \n",
        "    return {\n",
        "        \"fold_accuracies\": fold_accuracies,\n",
        "        \"overall_accuracy\": (all_y_true == all_y_pred).mean(),\n",
        "        \"confusion_matrix\": confusion_matrix(all_y_true, all_y_pred),\n",
        "        \"classification_report\": classification_report(\n",
        "            all_y_true, all_y_pred, target_names=CLASS_NAMES, output_dict=True\n",
        "        ),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate experiment directories\n",
        "print(\"Checking experiment directories...\\n\")\n",
        "valid_experiments = {}\n",
        "\n",
        "for name, path in EXPERIMENTS.items():\n",
        "    exp_path = Path(path)\n",
        "    if exp_path.exists():\n",
        "        summary = load_summary(exp_path)\n",
        "        if summary:\n",
        "            n_folds = summary.get(\"n_folds\", \"?\")\n",
        "            mean_acc = summary.get(\"mean_test_acc\", 0) * 100\n",
        "            std_acc = summary.get(\"std_test_acc\", 0) * 100\n",
        "            print(f\"✓ {name}: {n_folds} folds, {mean_acc:.1f}% ± {std_acc:.1f}%\")\n",
        "            valid_experiments[name] = exp_path\n",
        "        else:\n",
        "            print(f\"⚠ {name}: Directory exists but no summary found\")\n",
        "    else:\n",
        "        print(f\"✗ {name}: Directory not found ({path})\")\n",
        "\n",
        "if not valid_experiments:\n",
        "    print(\"\\n❌ No valid experiments found! Run training first:\")\n",
        "    print(\"   python -m src.train --config configs/eegnet.yaml --output-dir reports/loso/eegnet_experiment\")\n",
        "    print(\"   python -m src.train --config configs/atcnet.yaml --output-dir reports/loso/atcnet_experiment\")\n",
        "else:\n",
        "    print(f\"\\n✓ Found {len(valid_experiments)} valid experiment(s)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Preprocessed Data Visualization\n",
        "\n",
        "Visualize the preprocessed EEG data to understand what the models are learning from.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sample subject data\n",
        "sample_subject = \"A01T\"\n",
        "sample_path = DATA_DIR / f\"{sample_subject}.npz\"\n",
        "\n",
        "if sample_path.exists():\n",
        "    data = np.load(sample_path, allow_pickle=True)\n",
        "    X = data[\"X\"]  # (trials, channels, time)\n",
        "    y = data[\"y\"]\n",
        "    \n",
        "    n_trials, n_channels, n_samples = X.shape\n",
        "    sfreq = 250  # BCI Competition IV 2a sampling frequency\n",
        "    \n",
        "    print(f\"Subject: {sample_subject}\")\n",
        "    print(f\"Shape: {X.shape} (trials × channels × samples)\")\n",
        "    print(f\"Sampling frequency: {sfreq} Hz\")\n",
        "    print(f\"Trial duration: {n_samples / sfreq:.2f} seconds\")\n",
        "    print(f\"Class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
        "else:\n",
        "    print(f\"❌ Sample data not found at {sample_path}\")\n",
        "    X, y = None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X is not None:\n",
        "    # Select one trial per class\n",
        "    trial_left = X[np.where(y == 0)[0][0]]  # First left-hand trial\n",
        "    trial_right = X[np.where(y == 1)[0][0]]  # First right-hand trial\n",
        "    \n",
        "    time = np.arange(n_samples) / sfreq\n",
        "    \n",
        "    # Channel names for BCI Competition IV 2a (22 EEG channels)\n",
        "    channel_names = [\n",
        "        'Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'C5', 'C3', 'C1', 'Cz', 'C2',\n",
        "        'C4', 'C6', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'P1', 'Pz', 'P2', 'POz'\n",
        "    ]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 10))\n",
        "    \n",
        "    for ax, trial, label in zip(axes, [trial_left, trial_right], CLASS_NAMES):\n",
        "        # Plot each channel with offset for visibility\n",
        "        offset = 0\n",
        "        for i in range(n_channels):\n",
        "            ch_data = trial[i] - trial[i].mean()  # Center each channel\n",
        "            ax.plot(time, ch_data + offset, linewidth=0.5, color='#1a1a2e')\n",
        "            offset += np.abs(ch_data).max() * 2.5\n",
        "        \n",
        "        ax.set_xlabel('Time (s)')\n",
        "        ax.set_ylabel('Channel')\n",
        "        ax.set_title(f'{label} Motor Imagery', fontweight='bold')\n",
        "        ax.set_yticks(np.linspace(0, offset, n_channels))\n",
        "        ax.set_yticklabels(channel_names[:n_channels], fontsize=8)\n",
        "        ax.set_xlim(0, time[-1])\n",
        "    \n",
        "    plt.suptitle(f'EEG Time Series - Subject {sample_subject}', fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X is not None:\n",
        "    # Compute and plot spectrograms for motor-relevant channels (C3, Cz, C4)\n",
        "    motor_channels = {'C3': 7, 'Cz': 9, 'C4': 11}  # Indices for motor cortex channels\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "    \n",
        "    for row, (trial, label) in enumerate([(trial_left, \"Left Hand\"), (trial_right, \"Right Hand\")]):\n",
        "        for col, (ch_name, ch_idx) in enumerate(motor_channels.items()):\n",
        "            ax = axes[row, col]\n",
        "            \n",
        "            # Compute spectrogram\n",
        "            f, t, Sxx = signal.spectrogram(\n",
        "                trial[ch_idx], fs=sfreq, nperseg=64, noverlap=56\n",
        "            )\n",
        "            \n",
        "            # Plot only 4-40 Hz (relevant for motor imagery)\n",
        "            freq_mask = (f >= 4) & (f <= 40)\n",
        "            im = ax.pcolormesh(\n",
        "                t, f[freq_mask], 10 * np.log10(Sxx[freq_mask] + 1e-10),\n",
        "                shading='gouraud', cmap='viridis'\n",
        "            )\n",
        "            \n",
        "            ax.set_ylabel('Frequency (Hz)' if col == 0 else '')\n",
        "            ax.set_xlabel('Time (s)' if row == 1 else '')\n",
        "            ax.set_title(f'{ch_name} - {label}', fontweight='bold')\n",
        "            \n",
        "            # Highlight mu (8-12 Hz) and beta (18-26 Hz) bands\n",
        "            for band, (lo, hi) in [(\"μ\", (8, 12)), (\"β\", (18, 26))]:\n",
        "                ax.axhline(lo, color='white', linestyle='--', alpha=0.5, linewidth=0.5)\n",
        "                ax.axhline(hi, color='white', linestyle='--', alpha=0.5, linewidth=0.5)\n",
        "    \n",
        "    plt.colorbar(im, ax=axes, label='Power (dB)', shrink=0.8)\n",
        "    plt.suptitle(f'Spectrograms - Motor Cortex Channels ({sample_subject})', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Training Curves (Averaged Over Folds)\n",
        "\n",
        "Visualize training dynamics across all 9 LOSO folds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_curves(experiments: Dict[str, Path], metric_pairs: List[Tuple[str, str]]):\n",
        "    \"\"\"Plot training curves for multiple experiments.\n",
        "    \n",
        "    Args:\n",
        "        experiments: Dict mapping model name to experiment path\n",
        "        metric_pairs: List of (metric_name, display_name) tuples\n",
        "    \"\"\"\n",
        "    n_models = len(experiments)\n",
        "    n_metrics = len(metric_pairs)\n",
        "    \n",
        "    if n_models == 0:\n",
        "        print(\"No experiments to plot.\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(n_metrics, n_models, figsize=(6 * n_models, 4 * n_metrics), squeeze=False)\n",
        "    \n",
        "    for col, (model_name, exp_path) in enumerate(experiments.items()):\n",
        "        histories = load_fold_histories(exp_path)\n",
        "        color = get_model_color(model_name)\n",
        "        \n",
        "        for row, (metric, display_name) in enumerate(metric_pairs):\n",
        "            ax = axes[row, col]\n",
        "            \n",
        "            if not histories:\n",
        "                ax.text(0.5, 0.5, \"No data\", ha='center', va='center', transform=ax.transAxes)\n",
        "                continue\n",
        "            \n",
        "            epochs, mean, std = aggregate_histories(histories, metric)\n",
        "            \n",
        "            ax.plot(epochs, mean, color=color, linewidth=2, label=f'Mean ({len(histories)} folds)')\n",
        "            ax.fill_between(epochs, mean - std, mean + std, color=color, alpha=0.2, label='±1 Std')\n",
        "            \n",
        "            ax.set_xlabel('Epoch')\n",
        "            ax.set_ylabel(display_name)\n",
        "            ax.set_title(f'{model_name} - {display_name}', fontweight='bold')\n",
        "            ax.legend(loc='best', fontsize=9)\n",
        "            ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot loss and accuracy curves for all models\n",
        "if valid_experiments:\n",
        "    plot_training_curves(\n",
        "        valid_experiments,\n",
        "        metric_pairs=[\n",
        "            (\"train_loss\", \"Training Loss\"),\n",
        "            (\"val_loss\", \"Validation Loss\"),\n",
        "            (\"train_acc\", \"Training Accuracy\"),\n",
        "            (\"val_acc\", \"Validation Accuracy\"),\n",
        "        ]\n",
        "    )\n",
        "else:\n",
        "    print(\"No valid experiments found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Detailed Test Performance\n",
        "\n",
        "Per-fold accuracy, confusion matrices, and classification metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_fold_accuracies(experiments: Dict[str, Path]):\n",
        "    \"\"\"Plot per-fold test accuracies for each model.\"\"\"\n",
        "    n_models = len(experiments)\n",
        "    if n_models == 0:\n",
        "        print(\"No experiments to plot.\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(1, n_models, figsize=(6 * n_models, 5), squeeze=False)\n",
        "    \n",
        "    for idx, (model_name, exp_path) in enumerate(experiments.items()):\n",
        "        ax = axes[0, idx]\n",
        "        predictions = load_fold_predictions(exp_path)\n",
        "        \n",
        "        if not predictions:\n",
        "            ax.text(0.5, 0.5, \"No predictions found\", ha='center', va='center', transform=ax.transAxes)\n",
        "            continue\n",
        "        \n",
        "        metrics = compute_aggregate_metrics(predictions)\n",
        "        fold_accs = metrics[\"fold_accuracies\"]\n",
        "        \n",
        "        subjects = list(fold_accs.keys())\n",
        "        accs = [fold_accs[s] * 100 for s in subjects]\n",
        "        mean_acc = np.mean(accs)\n",
        "        \n",
        "        color = get_model_color(model_name)\n",
        "        bars = ax.bar(subjects, accs, color=color, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
        "        ax.axhline(mean_acc, color='#333', linestyle='--', linewidth=2, label=f'Mean: {mean_acc:.1f}%')\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for bar, acc in zip(bars, accs):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                    f'{acc:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        ax.set_xlabel('Held-out Subject')\n",
        "        ax.set_ylabel('Test Accuracy (%)')\n",
        "        ax.set_title(f'{model_name} - Per-Fold Accuracy', fontweight='bold')\n",
        "        ax.set_ylim(0, 110)\n",
        "        ax.legend(loc='lower right')\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if valid_experiments:\n",
        "    plot_fold_accuracies(valid_experiments)\n",
        "else:\n",
        "    print(\"No valid experiments found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrices(experiments: Dict[str, Path]):\n",
        "    \"\"\"Plot aggregated confusion matrices for each model.\"\"\"\n",
        "    n_models = len(experiments)\n",
        "    if n_models == 0:\n",
        "        print(\"No experiments to plot.\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(1, n_models, figsize=(5 * n_models, 4), squeeze=False)\n",
        "    \n",
        "    for idx, (model_name, exp_path) in enumerate(experiments.items()):\n",
        "        ax = axes[0, idx]\n",
        "        predictions = load_fold_predictions(exp_path)\n",
        "        \n",
        "        if not predictions:\n",
        "            ax.text(0.5, 0.5, \"No predictions found\", ha='center', va='center', transform=ax.transAxes)\n",
        "            continue\n",
        "        \n",
        "        metrics = compute_aggregate_metrics(predictions)\n",
        "        cm = metrics[\"confusion_matrix\"]\n",
        "        \n",
        "        # Normalize to percentages\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "        \n",
        "        sns.heatmap(\n",
        "            cm_normalized, annot=True, fmt='.1f', cmap='Blues',\n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
        "            ax=ax, cbar_kws={'label': '%'}\n",
        "        )\n",
        "        \n",
        "        # Add raw counts in smaller text\n",
        "        for i in range(2):\n",
        "            for j in range(2):\n",
        "                ax.text(j + 0.5, i + 0.75, f'(n={cm[i, j]})', \n",
        "                       ha='center', va='center', fontsize=8, color='gray')\n",
        "        \n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('True')\n",
        "        ax.set_title(f'{model_name}\\nAcc: {metrics[\"overall_accuracy\"]*100:.1f}%', fontweight='bold')\n",
        "    \n",
        "    plt.suptitle('Confusion Matrices (All Folds Aggregated)', fontsize=14, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if valid_experiments:\n",
        "    plot_confusion_matrices(valid_experiments)\n",
        "else:\n",
        "    print(\"No valid experiments found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_classification_reports(experiments: Dict[str, Path]):\n",
        "    \"\"\"Print classification reports for each model.\"\"\"\n",
        "    for model_name, exp_path in experiments.items():\n",
        "        predictions = load_fold_predictions(exp_path)\n",
        "        \n",
        "        if not predictions:\n",
        "            print(f\"\\n{model_name}: No predictions found\")\n",
        "            continue\n",
        "        \n",
        "        metrics = compute_aggregate_metrics(predictions)\n",
        "        report = metrics[\"classification_report\"]\n",
        "        \n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\" {model_name} - Classification Report\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"{'Class':<15} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}\")\n",
        "        print(\"-\" * 55)\n",
        "        \n",
        "        for class_name in CLASS_NAMES:\n",
        "            cls = report[class_name]\n",
        "            print(f\"{class_name:<15} {cls['precision']:>10.3f} {cls['recall']:>10.3f} {cls['f1-score']:>10.3f} {int(cls['support']):>10}\")\n",
        "        \n",
        "        print(\"-\" * 55)\n",
        "        acc = report['accuracy']\n",
        "        macro = report['macro avg']\n",
        "        print(f\"{'Accuracy':<15} {'':<10} {'':<10} {acc:>10.3f} {int(report['weighted avg']['support']):>10}\")\n",
        "        print(f\"{'Macro Avg':<15} {macro['precision']:>10.3f} {macro['recall']:>10.3f} {macro['f1-score']:>10.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if valid_experiments:\n",
        "    print_classification_reports(valid_experiments)\n",
        "else:\n",
        "    print(\"No valid experiments found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Model Comparison Dashboard\n",
        "\n",
        "Direct side-by-side comparison of all models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_comparison_bar_chart(experiments: Dict[str, Path]):\n",
        "    \"\"\"Create grouped bar chart comparing per-fold accuracy across models.\"\"\"\n",
        "    if len(experiments) < 2:\n",
        "        print(\"Need at least 2 experiments for comparison.\")\n",
        "        return\n",
        "    \n",
        "    # Collect data\n",
        "    model_data = {}\n",
        "    all_subjects = set()\n",
        "    \n",
        "    for model_name, exp_path in experiments.items():\n",
        "        predictions = load_fold_predictions(exp_path)\n",
        "        if predictions:\n",
        "            metrics = compute_aggregate_metrics(predictions)\n",
        "            model_data[model_name] = metrics[\"fold_accuracies\"]\n",
        "            all_subjects.update(metrics[\"fold_accuracies\"].keys())\n",
        "    \n",
        "    if not model_data:\n",
        "        print(\"No prediction data found.\")\n",
        "        return\n",
        "    \n",
        "    subjects = sorted(all_subjects)\n",
        "    n_models = len(model_data)\n",
        "    n_subjects = len(subjects)\n",
        "    \n",
        "    # Create grouped bar chart\n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "    \n",
        "    bar_width = 0.8 / n_models\n",
        "    x = np.arange(n_subjects)\n",
        "    \n",
        "    for i, (model_name, fold_accs) in enumerate(model_data.items()):\n",
        "        accs = [fold_accs.get(s, 0) * 100 for s in subjects]\n",
        "        offset = (i - n_models / 2 + 0.5) * bar_width\n",
        "        color = get_model_color(model_name)\n",
        "        \n",
        "        bars = ax.bar(x + offset, accs, bar_width, label=model_name, color=color, \n",
        "                      alpha=0.85, edgecolor='black', linewidth=0.5)\n",
        "    \n",
        "    ax.set_xlabel('Held-out Subject', fontsize=12)\n",
        "    ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "    ax.set_title('Per-Subject Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(subjects)\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.legend(loc='lower right', fontsize=10)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(valid_experiments) >= 2:\n",
        "    plot_comparison_bar_chart(valid_experiments)\n",
        "elif len(valid_experiments) == 1:\n",
        "    print(\"Only one experiment found. Run training for another model to enable comparison.\")\n",
        "else:\n",
        "    print(\"No valid experiments found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_aggregate_summary_table(experiments: Dict[str, Path]):\n",
        "    \"\"\"Display summary statistics table for all models.\"\"\"\n",
        "    if not experiments:\n",
        "        print(\"No experiments found.\")\n",
        "        return\n",
        "    \n",
        "    rows = []\n",
        "    \n",
        "    for model_name, exp_path in experiments.items():\n",
        "        summary = load_summary(exp_path)\n",
        "        predictions = load_fold_predictions(exp_path)\n",
        "        \n",
        "        if summary:\n",
        "            row = {\n",
        "                \"Model\": model_name,\n",
        "                \"Folds\": summary.get(\"n_folds\", \"?\"),\n",
        "                \"Mean Acc\": f\"{summary.get('mean_test_acc', 0)*100:.2f}%\",\n",
        "                \"Std\": f\"±{summary.get('std_test_acc', 0)*100:.2f}%\",\n",
        "                \"Min\": f\"{summary.get('min_test_acc', 0)*100:.2f}%\",\n",
        "                \"Max\": f\"{summary.get('max_test_acc', 0)*100:.2f}%\",\n",
        "            }\n",
        "            \n",
        "            if predictions:\n",
        "                metrics = compute_aggregate_metrics(predictions)\n",
        "                report = metrics[\"classification_report\"]\n",
        "                row[\"Macro F1\"] = f\"{report['macro avg']['f1-score']:.3f}\"\n",
        "            else:\n",
        "                row[\"Macro F1\"] = \"N/A\"\n",
        "            \n",
        "            rows.append(row)\n",
        "    \n",
        "    if not rows:\n",
        "        print(\"No summary data available.\")\n",
        "        return\n",
        "    \n",
        "    # Print as formatted table\n",
        "    print(\"\\n\" + \"=\" * 85)\n",
        "    print(\" MODEL COMPARISON SUMMARY\")\n",
        "    print(\"=\" * 85)\n",
        "    \n",
        "    headers = [\"Model\", \"Folds\", \"Mean Acc\", \"Std\", \"Min\", \"Max\", \"Macro F1\"]\n",
        "    header_fmt = f\"{'Model':<15} {'Folds':>6} {'Mean Acc':>10} {'Std':>10} {'Min':>10} {'Max':>10} {'Macro F1':>10}\"\n",
        "    print(header_fmt)\n",
        "    print(\"-\" * 85)\n",
        "    \n",
        "    for row in rows:\n",
        "        print(f\"{row['Model']:<15} {row['Folds']:>6} {row['Mean Acc']:>10} {row['Std']:>10} {row['Min']:>10} {row['Max']:>10} {row['Macro F1']:>10}\")\n",
        "    \n",
        "    print(\"=\" * 85)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_aggregate_summary_table(valid_experiments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_overlaid_training_curves(experiments: Dict[str, Path], metric: str, display_name: str):\n",
        "    \"\"\"Plot training curves for all models on the same axes.\"\"\"\n",
        "    if not experiments:\n",
        "        print(\"No experiments found.\")\n",
        "        return\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    for model_name, exp_path in experiments.items():\n",
        "        histories = load_fold_histories(exp_path)\n",
        "        \n",
        "        if not histories:\n",
        "            continue\n",
        "        \n",
        "        epochs, mean, std = aggregate_histories(histories, metric)\n",
        "        color = get_model_color(model_name)\n",
        "        \n",
        "        ax.plot(epochs, mean, color=color, linewidth=2.5, label=model_name)\n",
        "        ax.fill_between(epochs, mean - std, mean + std, color=color, alpha=0.15)\n",
        "    \n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel(display_name, fontsize=12)\n",
        "    ax.set_title(f'{display_name} Comparison (Mean ± Std across folds)', fontsize=14, fontweight='bold')\n",
        "    ax.legend(loc='best', fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if valid_experiments:\n",
        "    plot_overlaid_training_curves(valid_experiments, \"val_acc\", \"Validation Accuracy\")\n",
        "    plot_overlaid_training_curves(valid_experiments, \"val_loss\", \"Validation Loss\")\n",
        "else:\n",
        "    print(\"No valid experiments found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Next Steps\n",
        "\n",
        "To add a new model to this comparison:\n",
        "\n",
        "1. **Create a config file** (e.g., `configs/shallowconv.yaml`)\n",
        "\n",
        "2. **Run training with explicit output directory:**\n",
        "   ```bash\n",
        "   python -m src.train --config configs/shallowconv.yaml --output-dir reports/loso/shallowconv_experiment\n",
        "   ```\n",
        "\n",
        "3. **Add to EXPERIMENTS dict** at the top of this notebook:\n",
        "   ```python\n",
        "   EXPERIMENTS = {\n",
        "       \"EEGNet\": \"../reports/loso/eegnet_experiment\",\n",
        "       \"ATCNet\": \"../reports/loso/atcnet_experiment\",\n",
        "       \"ShallowConvNet\": \"../reports/loso/shallowconv_experiment\",  # New!\n",
        "   }\n",
        "   ```\n",
        "\n",
        "4. **Re-run all cells** to see the new model in comparisons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
